---
title: "Inequality in measuring scholarly success: Variation in the *h*-index within and between disciplines"
shorttitle: "Variation in the *h*-index"
abstract: Scholars and university administrators have a vested interest in building equitable valuation systems of academic work for both practical (e.g., resource distribution) and more lofty purposes (e.g., what constitutes “good” research). Well-established inequalities in science pose a difficult challenge to those interested in constructing a parsimonious and fair method for valuation as stratification occurs within academic disciplines, but also between them. The *h*-index, a popular research metric, has been formally used as one such method of valuation. In this article, we use the case of the *h*-index to examine how the distribution of research metrics reveal within and between discipline inequalities. Using data from over 50,000 high performing scientists across 174 disciplines, we construct random effects within-between models predicting the *h*-index. Results suggest significant within-discipline variation in several forms, specifically sole-authorship and female penalties, as well as significant between discipline variation in terms of sole authorship. Field-specific models emphasize the "apples-to-oranges" property of cross-discipline comparison. Conclusions include continued caution when using the *h*-index or similar metrics for valuation purposes.
keywords: [Metrics, h-index, Gender Inequality, Team Science, Higher Education]
thanks: Thanks to everyone for checking this out.
reference-section-title: References
bibliography: ../bibliography/project.bib
csl: ../bibliography/chicago-parenthetical.csl
format:
  aog-article-pdf:
    keep-tex: false
    include-in-header: 
      text: |
        \usepackage{dcolumn}
#  submittable-pdf:
#     keep-tex: false
#     fig-pos: "!t"
#     include-in-header: 
#       text: |
#         \usepackage{dcolumn}
  #submittable-docx: default
---

```{r}
#| label: setup
#| include: false
library(here)
source(here("utils","check_packages.R"))
load(here("data","data_constructed","analytical_data.RData"))
```

```{r}
#| label: functions

# a function to pool the models and collect important stats
pool_models <- function(models) {
  
  # first use mice::pool
  pooled <- mice::pool(models)
  
  # get the within and between residual standard deviation - a pain with lme4
  vars <- apply(sapply(models, function(x) {as.data.frame(VarCorr(x))$sdcor}), 
               1, mean)
  
  return(list(coef=summary(pooled),
              BIC=mean(pooled$glanced$BIC),
              nobs=mean(pooled$glanced$nobs),
              ngrps=as.numeric(ngrps(models[[1]])),
              sd_between=vars[1],
              sd_within=vars[2]))
}

# a function to convert model results into something texreg knows what to do
# with
convert_model <- function(model) {
  
  tr <- createTexreg(
    coef.names = as.character(model$coef$term), 
    coef = model$coef$estimate, 
    se = model$coef$std.error, 
    pvalues = model$coef$p.value,
    gof.names = c("Residual SD between discipline", 
                  "Residual SD within discipline",
                  "N (discipline)",
                  "N (individual)"),
    gof = c(model$sd_between, 
            model$sd_within, 
            model$ngrps, 
            model$nobs),
    gof.decimal = c(TRUE,TRUE, FALSE, FALSE)
  )
}

```

# Introduction

From teaching to research, systematic evaluation of academic work presents a unique set of challenges as the academic disciplines that broadly organize scholarly labor differ upon several relevant factors. Differences in demographic characteristics, like race and gender, capture processes of racism and sexism that limit the opportunities for some scholars and not others across disciplines [@hofstra_diversity_2020; @lariviere_bibliometrics_2013; @xie_sex_1998]. Cultural and economic differences between disciplines, like the prevalence of team science or even the definition of success, can affect the quality and quantity of publication [@fortunato_science_2018; @gardner_conceptualizing_2009; @stephan_how_2012]. Scholars have invented dozens of metrics that are formally or informally used in the evaluation of scholarly research, despite warnings about apples-to-oranges comparisons given within discipline and between discipline differences [@hicks_bibliometrics_2015; @ioannidis_standardized_2019]. The most widely used metric of this type is the *h*-index, a simple score where a scholar with 10 published articles with at least 10 citations receives a *h*-index of 10.[^1] The limitations of the *h*-index are well understood, but less is known about how processes of inequality are embedded both within and between disciplines. Here, we ask three related questions:

[^1]: We somewhat reluctantly use the "*h*-index" label throughout for the metric introduced by @hirsch_index_2005 while acknowledging this anonymization abstracts the sociohistorical context in which it was created. We occasionally offer the Hirsch Index to maintain Jorge Hirsch's role in its creation.

1.  What is the variation within and between disciplines in the *h*-index?
2.  What within and between discipline factors contribute to this variation?
3.  How do within and between discipline factors vary across fields (e.g., social sciences, medical sciences, natural sciences)?

These questions arise from at least two important considerations. First, like other forms of work, academic labor is beset by inequalities. A large body of research describes the factors that contribute to inequality in academia [see @fox_gender_2017; @long_scientific_1995 for reviews]. From well-known pipeline effects to publication differences to varying effects of parenthood, scholars have shown the multifaceted ways some scholars are obstructed from paths to success at work [e.g. @fox_gender_2005; @grant_revisiting_2000; @morgan_unequal_2021 among others]. At the same time, differences exist in the distribution of resources to disciplines as can be seen in variation in federal funding and the way resources are distributed to departments, the proxies for disciplines on university campuses [@katz_metrics_2020]. Second, numerous scholars have drawn attention to the increasing push to quantify aspects of social life [@mennicken_what_2019]. Research on the risks of quantification points to the crude ways that quantification reduces complex social phenomena, often with implications for inequality and nearly always with implications for valuation, or what is considered good or bad. In the case of scholarly metrics, critics have raised concerns about how these metrics transform scholarship into a capitalist-like market (e.g., neoliberalization), build unproductive individual constraints into academic labor - such as the anxiety connected to hyper-competitiveness - and ultimately result in less productive and innovative scholarship [@edwards_academic_2017; @muller_tyranny_2019] and efforts to "game metrics" at either the journal or individual-level [@siler_who_2022].[^2] Turning to within and between discipline variation in the *h*-index offers insight into how metrics relate to inequalities and contributes to research on the limitations of simplifying summaries of academic labor through quantification.

[^2]: [@azoulay_selfcitation_2020] provide evidence that self-citation -- one way of potentially "gaming" metrics is not itself gendered.

While prior work examines the *h*-index by field, discipline, or gender, less is known about how within and between discipline differences affect *h*-index scores in relationship to one another <!-- TODO: Add citations to Bihari, Tripathi, Deepak 2023 and Iglesias and Pecharroman 2007 -->. To understand factors that contribute to differences in scholar's *h*-index, or Hirsch index, we use data on over 50,000 high-performing scientists across 174 disciplines in the United States. This analysis draws from Ioannidis et al.'s [-@ioannidis_standardized_2019] dataset identifying the top scientists in the Scopus database. To understand within and between discipline differences, we use random effects within-between (REWB) models predicting the *h*-index with a range of factors including disciplinary age, the number of sole publications, a female name index, and a measure of university productivity. The REWB model accounts for distinct within and between disciplinary effects on *h*-index scores, consistent with calls to develop multilevel analyses of gender disparities in science [@fox_gender_2020]. Results indicate that substantial within discipline differences exist, including both female and sole authorship penalties. Results further show substantial between discipline differences related to sole authorship. Moreover, roughly a third of the variation in *h*-index is between rather than within disciplines, suggesting an important apples-to-oranges problem when comparing the *h*-index across disciplines. This apples-to-oranges problem results in significant differences across fields as well, especially in terms of sole authorship. Conclusions highlight how the continued use of the *h*-index or similar metrics for valuation <!-- REVIEW: three p words in a row makes this a bit of a tongue twister. I would propose getting rid of either the purposes (becasue valuation I think works fine on its own) or potentially (because its overly hedging, we actually think this *does* perpetuate right?) or both --> perpetuates inequality in science both within and between disciplines.

# Inequality in Science

Inequality in science, and in academia generally, persists within and between disciplines and along well-known axes. Gender is one of the most studied axes of inequality in science. Research on the scientific pipeline, for example, shows how obstacles, like cultural stereotypes about skills differences in math and science and related social psychological factors such as a sense of belonging, limit pathways to particular scientific fields for girls [@cech_professional_2011; @ma_math_2021; @penner_men_2019]. These pipeline factors continue throughout the research life course as men and women become segregated in doctoral programs by field and prestige [@weeden_degrees_2017a]. When these significant obstacles are overcome, women continue to experience inequality in academic work, including in publication. Publication is both the outcome of academic labor and the currency of academic careers. How many and the type of publications produced by a scientist translates into tangible resources, like salary raises and the job security of tenure, and less tangible resources, like prestige. While significant gains have been made by women in academic work - numerous fields that were male dominated in the mid- to late-twentieth century are now majority female - publication remains a site of persistent inequality [@xie_sex_1998].[^3]

[^3]: Early signs indicate that these forms of gender inequality may have increased due to the COVID-19 pandemic or a "pandemic penalty" [@king_pandemic_2021].

Sociologists of science have spent decades trying to disentangle the factors associated with gender differences in publication, especially related to differences in the number of publications and citations. Nearly 40 years ago, @cole_productivity_1984 referred to the ongoing male advantage in publication and citation counts as the "productivity puzzle" because the causes of this advantage remained difficult to pinpoint. They conclude, "\[S\]ince gender differences in published productivity persist, the productivity puzzle has yet to be solved" (p. 250). While women have made significant gains in many academic fields, research suggests that the productivity puzzle remains. For example, Leahey [-@leahey_gender_2006; -@leahey_not_2007] shows how the level of research specialization interacts with gender to affect productivity, with consequences for earnings. More recently, @huang_historical_2020 provide evidence that the productivity puzzle in STEM fields results from variation between men and women in career length and exit rates as productivity appears to be more equal across shorter time horizons. Scholars have also turned to more complex mechanisms that may help perpetuate gender and racial hierarchies in academic work. For example, using data on US doctoral recipients, @hofstra_diversity_2020 find that gender and racial minorities are more likely to generate innovative scientific work, but that this work is less likely to be adopted by future researchers with consequences for academic hiring.

A prestige puzzle may also coincide with the productivity puzzle as women may be less likely to publish in the top or most prestigious journals in their fields [@light_gender_2013]. As top journals have higher impact factors, the prestige puzzle could have a significant impact on differences in publication metrics and career outcomes. This form of inequality may occur for a variety of reasons including a lack of mentorship, different family and work-based responsibilities between men and women, and differences in specialization similar to Leahey's [-@leahey_gender_2006; -@leahey_not_2007] work on specialization and productivity [@light_gender_2009]. Drawing on literature on occupational segregation and identity, @light_gender_2013 shows how the prestige puzzle has changed over time within sociology. Earlier cohorts of women sociologists were significantly less likely to publish in top sociology journals compared to men regardless of specialty areas. However, as more women entered sociology, occupational segregation occurred with subfields becoming sharply gender imbalanced. While baseline models of the prestige puzzle for more recent cohorts reveal the persistence of this form of inequality, more complete models that control for specialization, or occupational identity, show that the contemporary effect likely operates through these segregation processes. More recent work by @lynn_rare_2019 on sociology, economics, and political science shows a null effect of gender on citation when social scientists are situated in similar disciplinary and sub-field spaces, suggesting that teasing apart the contexts when a gender penalty persists and when it does not remains an important concern for those interested in inequality.

Collaboration also likely plays a role with significant historical differences in coauthorship networks based on gender [@moody_structure_2004]. Recent research on computer scientists finds significant differences in collaboration between men and women. Men are more likely to have larger coauthorship networks and to play brokerage roles within them, while women's networks exhibit increasing gender homophily [@jadidi_gender_2018]. These differences relate to publication outcomes as the network factors positively affect measures of productivity, including the *h*-index. @jadidi_gender_2018 conclude that women "on average are less likely to adapt to the collaboration patterns that are related with success. However, those women who become successful computer scientists exhibit the same collaborative behavior as their successful male colleagues." Research in both political science and sociology also identifies how team science affects gender and publication in these disciplines pointing to how the structure of scientific work can negatively impact women social scientists [@akbaritabar_gender_2021; @teele_gender_2017].

Collaboration may impact individual-level inequalities beyond gender. Collaboration is broadly understood as a key aspect of epistemic culture. Epistemic culture, as defined by @cetina_epistemic_1999 consists of "those amalgams of arrangements and mechanisms...which, in a given field, make up how we know what we know" <!-- TODO: Add page 1 to the citation for the above Cetina quote -->[see also @cronin_scholarly_2003]. The questions that scholars ask and the strategies that they use to answer them differ by field with consequences for individuals embedded in specific cultures. Plainly, the structure of team science matters for publication outcomes. This suggests that within discipline differences may affect publication metrics, like the *h*-index, but also points to how inequality may occur between disciplines as pronounced differences exist between disciplines in terms of factors like gender composition and team versus sole authorship.

## Inequality Between Disciplines and Fields

Disciplinary differences affect inequality along several dimensions. For example, differences in gender composition may have direct and indirect effects on how resources are distributed in universities. Disciplinary cultures also differ and these differences may affect inequality. For example, disciplines differ in terms of how work is evaluated [@lamont_how_2009] or even how emotions are expressed at work [@koppman_joy_2015]. Moreover, disciplines differ in terms of how academic work is conducted [@huang_historical_2020]. Do scholars collaborate in teams or are they more likely to work alone? Less is known about how these between discipline-based inequalities differ from within discipline inequality. Research on collaboration and citation impact using the *h*-index shows disciplinary differences in the effect that collaboration has on impact with more collaboration having a stronger positive effect in physics and medicine, while having a smaller effect in the brain sciences or computer science [@parish_dynamics_2018]. Disciplinary differences occur regarding more tangible resources, like federal funding. @lanahan_domino_2016 describe the substantial differences between fields in terms of federal funding with implications - a "domino effect" - for future non-federal funding and a potential site of cumulative advantage stratifying disciplines [@lynn_15_2021].

One of the ways that disciplines differ and also one of the ways that disciplines may be valued differently is how they perform on commonly used metrics. Time-worn debates in the philosophy of science have tried to identify the implications of and/or reconcile the so-called "two cultures" or the division between the arts and the sciences (see X) <!-- TODO: Add CP Snow's The Two Cultures-->. While this debated formulation may exaggerate differences, the two cultures perspective draws attention to the variation that occurs across fields both at a philosophical-level, but also as a more practical concern. In terms of the latter, prior work has suggested field normalizing the *h*-index to account for between field variation (see X) <!-- TODO: Add Bi's four problems article-->. Questions remain about the extent of variation within and between fields in addition to the variation occurring within and between disciplines situated in fields. Do field-level variations in gender composition and team science result in apple-to-oranges comparisons when using common scholarly metrics, like the *h*-index?

# The Risks of Quantification

Quantification has become a central feature of contemporary life as "\[a\]dministration, management, and even mundane daily activities are increasingly structured around performance measures, cost-benefit analysis, risk calculations, ratings, and rankings" [@mennicken_what_2019, p. 224]. Critiques often focus on the risks of quantification as a central factor in determining worth, which occurs in a variety of fields from the law to business to education [@muller_tyranny_2019, p. 4]. This "metrics fixation" is part of the broader process of neoliberalization of education. Neoliberalization succinctly captures an effort to "economize everything" [@berg_producing_2016, p. 171], such that neoliberal reason becomes common sense or simply the default rationale people use to make decisions. Metrics reinforce the notion that individual performance at work can be easily calculated and compared; therefore, material rewards like promotions and raises can be fairly and transparently applied. Of course, metrics often hide as much as they reveal as they simplify a process by carving away essential components. In universities, the metrics that help reinforce and are reinforced by neoliberal reason summarize entire careers for administrators who likely have little to no understanding of the research that the metrics summarize. This disempowers colleagues within and outside of a scholar's university, while centralizing power in the hands of bureaucrats with little incentive to actually understand academic work with which they are inexpert.

University administrators' use of metrics to evaluate faculty output is a fairly recent phenomenon. Prior to the development of bibliometric indicators, evaluation of scholarly research was performed primarily by disciplinary specialists who offered qualitative assessment of a research record. While peer assessment is still a central part of evaluation processes, metrics such as citation counts, journal impact factors, and the *h*-index are now commonly incorporated into hiring and promotion decisions [@mckiernan_metaresearch_2019] and are seen as more important to more vulnerable untenured scholars than tenured ones [@niles_why_2020]. Qualitative expert assessment that involves hours of engagement with the work summarized by metrics may be devalued by administrators motivated by a logic that privileges competition, central decision-making, and market valuation [@berg_producing_2016].

Critics have raised concerns about how metrics transform scholarship into a capitalist-like market at the core of neoliberalization resulting in "perverse incentives" for researchers to publish shoddy or fraudulent work [@edwards_academic_2017], while simultaneously resulting in mental health trauma for academic workers experiencing hypercompetitive markets and suspicious management [@forrester_mental_2021]. Administrators' continued reliance upon metrics serves as a modern form of Taylorism, a production principle used in the early twentieth century that pursued technological solutions to the "problem" of worker-related inefficiencies on the shop floor with little regard to employee satisfaction or wellness [@braverman_labor_1998]. By using technology to set the nature and pace of production, owners and managers gain greater control over the labor process itself. Prioritizing metrics creates a demand for quantity over quality, and by following these demands academic laborers risk surrendering some degree of control over their own labor processes.

## The Case of the h-index

One key metric used for evaluation purposes is the *h*-index or Hirsch Index. Physicist Jorge Hirsch proposed the *h*-index as a "useful index to characterize the scientific output of a researcher" in a 2005 article in the Proceedings of the National Academy of Sciences [@hirsch_index_2005]. While acknowledging the "potentially distasteful" use of metrics for evaluation, he presents quantification as an economical means of evaluating impact. In this highly cited article, Hirsch defines the *h*-index as follows: "A scientist has index *h* if *h* of his or her Np papers have at least *h* citations each and the other (Np -- h) papers have ≤*h* citations each" (p. 16569). In other words, a scholar with 10 of their 100 publications with a citation count of 10 or higher will have an *h*-index of 10. He goes on to specify - again with some acknowledgement that metrics offer a "rough approximation" of a research portfolio - how and when the index could be put to use: "Based on typical h and m values found, I suggest (with large error bars) that for faculty at major research universities, h ≈ 12 might be a typical value for advancement to tenure (associate professor) and that h ≈ 18 might be a typical value for advancement to full professor" (p. 16571). In sum, this publication announced a simple means of evaluating research impact and permission to use the metric for evaluation purposes.

The immediate response to the *h*-index was largely positive with features in top scientific journals; however, some criticism of the index also quickly appeared [@barnes_hindex_2017]. Critics identified a range of issues from the relationship between the *h*-index and career length as well as the effect of self-citation [see @kelly_index_2006; @purvis_index_2006 among others]. However, the *h*-index and variants have proven enormously popular both in the bibliometrics and science of science communities and among university administrators seeking quick and cheap ways to evaluate scholars, including universities and science funding agencies [@barnes_hindex_2017]. The *h*-index is included as a key quantitative metric for annual review and/or tenure and promotion in faculty handbooks in a range of departments and schools in the United States (c.f., handbooks from the Boston University School of Public Health [@bostonuniversity_boston_2018], the Ohio State University Department of Surgery [@ohiostateuniversity_department_2014] or Oregon State University's College of Business [@oregonstateuniversity_oregon_2020]. Survey research in Germany on whether and how scholars understand the importance of the *h*-index indicate that natural scientists widely understand the importance of the *h*-index to their careers, but scholars in the humanities and social sciences do not [@kamrani_researchers_2021]. This variation is unfortunate as quantitative metrics are widely applied in German universities as elsewhere. In sum, despite some criticism, the *h*-index has been widely adopted although perhaps not widely understood. This analysis contributes to the broader literature on metrics and science by examining the factors contributing to within and between discipline variation in the *h*-index.

# Hypotheses

We develop the following hypotheses to better understand within and between discipline differences in the *h*-index, or Hirsch Index, based on the prior literature. Consistent with work on gender inequality and science and particularly work on the productivity and prestige puzzles, we examine the following:

## Within Discipline Hypotheses

In light of research on team science and its impact on academic careers, we examine the following:

> *Female Penalty Hypothesis (H1):* Authors with names more common among women will have lower *h*-index scores, on average.

> *Sole Author Hypothesis (H2):* Authors with more sole-authored publications will have lower *h*-index scores, on average.

## Between Disciplines Hypotheses

Although less well understood, based on the research describing disciplinary differences in culture, such as propensity to collaborate, and material differences in resources, we examine three related hypotheses:

> *Disciplinary Differences Hypothesis (H3a):* A substantial amount of the variation in the *h*-index likely occurs between disciplines.

> *Feminized Discipline Hypothesis (H3b):* Disciplines with a higher share of women will have lower *h*-index scores, on average.

> *Teamwork Variation Hypothesis (H3c):* Disciplines with a higher share of sole authorship will have lower *h*-index scores, on average.

## Field-level Hypothesis

Finally, a field-level view may help disentangle cultural and compositional effects and shed light on the question of "apples-to-oranges" comparison within and between fields. We, therefore, evaluate the following hypothesis:

> *Field Variation Hypothesis (H4):* Significant field differences will exist in terms of within and between disciplinary differences in the *h*-index.

# Data and Methods

To evaluate these hypotheses, we analyze data on high performing scholars according to well-known metrics. @ioannidis_standardized_2019 collected author-level bibliometrics on 100,000 high performing scholars from the Scopus database. They updated this data through 2019 and expanded to include the top 2% of authors overall across a wide range of disciplines [@ioannidis_updated_2020]. We use the most up-to-date data for our analyses. These data suffer from several limitations, including coverage differences between disciplines within the Scopus database [@mongeon_journal_2016; @singh_journal_2021]. Nonetheless, these well-curated data represent a unique opportunity to evaluate within and between disciplinary differences in *h*-index scores. We also see these data as offering a conservative test of such differences as variation is likely to widen when moving beyond scholars who are in the top 2% on these metrics.

We reduce the data along several dimensions to address some of the limitations of the Scopus database and for analytic purposes. First, we limit the analysis only to the 68,016 scholars in the United States to account for geographic variation in the Scopus database and geographic variation in how universities are structured. We then further restrict the analytical data to those who first published in 1960 or after and those who last published in 2017 or later to identify active scholars and reduce the impact of historical, rather than contemporary, patterns. These reductions, along with a small number of missing values on the variables below, leave us a final analytical sample size of 54,825 scholars nested in 174 disciplines.

The dependent variable for the analysis is the *h*-index. The key independent variables are the probability of female name and the percentage of sole authored publications. We also include additional control variables of the count of scholars from the same university in the same dataset to measure highly productive university environments, a specialization score measured as the proportion of total articles appearing in the main discipline for each author, and career length, measured by the years between first and last publication.

Estimating gender is problematic for numerous reasons including the typical reliance on government-provided data that often assumes and contributes to the gender binary [@mihaljevic_reflections_2019], and these methods should be used with caution and only when necessary. In this case, names are our only means of estimating gender. Here, we draw on first names data from the 1940-1990 US Social Security Administration to assign a probability of female name using the gender package in R [@blevins_jane_2015]. We use this probability directly in our models, rather than assigning an arbitrary cutoff for a binary gender assignment. A substantial number of cases (18%) are missing on this variable, usually because they were identified by initials rather than a full first name. Rather than lose this many cases, we use multiple imputation to assign values to the probability of female name based on the respondent's other variables, including discipline. We impute five complete datasets and pool analyses across these datasets for all of the models presented here.

We also consider how the patterns we observe may differ within large fields among disciplines. To explore this issue, we divide the total 174 disciplines into five large fields of the humanities, medical (including public health), professional, social sciences (including policy), and STEM. These fields differ from the fields provided in the original data but more closely correspond to the division of disciplines within the American academy. The supplementary materials show a full list of disciplines, which disciplines were assigned to each field, and summary statistics for each discipline.

@tbl-desc presents the descriptive statistics for the variables used in the analyses for the total sample and across the five different fields. Differences across fields in both the *h*-index and independent variables are notable. For example, scholars in the humanities have the highest average probability of a female name at 31.1%, while scholars in the STEM field have the lowest average probability at 13.5%. Scholars in the humanities also have the highest percentage of sole authored work at 57.8% while scholars in the medical fields have the lowest with only 8.1%. The much lower mean *h*-index among scholars in the humanities (19.1) relative to other disciplines is consistent with the expectation of gender differences and a penalty for sole-authored work. However, to more accurately gauge these relationships we turn to a modeling strategy that can estimate associations both within and between disciplines.

```{r}
#| label: tbl-desc
#| tbl-cap: Mean and standard deviation on key variables for the whole sample and by field, based on first complete dataset

# It is not fun to format this kind of descriptive stat table and get it to fit, 
# but here we go, with extensive use of glue and format

fmt_num <- function(value, decimals=1) {
  format(round(value, decimals), nsmall=decimals, big.mark=",")
}

tbl_desc <- scholars_imp[[1]] |>
  group_by(big_field) |>
  summarize(`h-index`=glue("{fmt_num(mean(h_index))} ({fmt_num(sd(h_index))})"),
            `% female`=glue("{fmt_num(100*mean(prop_female))} ({fmt_num(100*sd(prop_female))})"),
            `% sole author`=glue("{fmt_num(100*mean(prop_sole))} ({fmt_num(100*sd(prop_sole))})"),
            `career length`=glue("{fmt_num(mean(age))} ({fmt_num(sd(age))})"),
            `specialization`=glue("{fmt_num(mean(specialization), 2)} ({fmt_num(sd(specialization), 2)})"),
            `uni. count`=glue("{fmt_num(mean(uni_pub_cnt), 0)} ({fmt_num(sd(uni_pub_cnt), 0)})"),
            N=fmt_num(n(), 0))

tbl_desc <- scholars_imp[[1]] |>
  summarize(`h-index`=glue("{fmt_num(mean(h_index))} ({fmt_num(sd(h_index))})"),
            `% female`=glue("{fmt_num(100*mean(prop_female))} ({fmt_num(100*sd(prop_female))})"),
            `% sole author`=glue("{fmt_num(100*mean(prop_sole))} ({fmt_num(100*sd(prop_sole))})"),
            `career length`=glue("{fmt_num(mean(age))} ({fmt_num(sd(age))})"),
            `specialization`=glue("{fmt_num(mean(specialization), 2)} ({fmt_num(sd(specialization), 2)})"),
            `uni. count`=glue("{fmt_num(mean(uni_pub_cnt), 0)} ({fmt_num(sd(uni_pub_cnt), 0)})"),
            N=fmt_num(n(), 0)) |>
  mutate(big_field="All") |>
   relocate(big_field) |>
   bind_rows(tbl_desc)

# everything needs to get flipped around if we want it to fit
tbl_desc <-  as_tibble(cbind(Variable = names(tbl_desc)[-1], t(tbl_desc)[-1,])) |>
  rename(All=V2, Humanities=V3, Medical=V4, "Prof."=V5,
         `Soc. Sci.`=V6, STEM=V7)

tbl_desc |> gt() |>
  tab_spanner(label="Fields", 
              columns = c(Humanities, Medical, `Prof.`, `Soc. Sci.`,
                          STEM)) |>
  tab_source_note(source_note = "Note: standard deviations shown in parenthesis")

```

# Analytic Strategy

We model variation in *h*-score indices across scholars using random effects within-between (REWB) models [@bell_fixed_2019]. REWB models are a variant of multilevel models that allows researchers to estimate the effect of a given predictor variable both within (as per a standard "fixed effects" model) and between the higher level clusters (in this case disciplines). In general, the structure of the REWB model is:

$$y_{ij}=\beta_{0j}+\beta_1(x_{ij}-\bar{x}_{.j})+\beta_2(\bar{x}_{.j})+\upsilon_{0j}+\epsilon_{ij}$$

Where $y_{ij}$ is the outcome for the $i$th unit in the $j$th cluster and $x_{ij}$ is the predictor variable for the $i$th unit in the $j$th cluster. $\upsilon_{0j}$ and $\epsilon_{ij}$ are cluster-level and individual-level random errors, respectively. Because the mean of the $x$ for cluster $j$ ($\bar{x}_{.j}$) is included in the model and the $x_{ij}$ values are mean centered by cluster, the $\beta_1$ parameter is identical to that of a fixed-effects model in which all between variance is absorbed by cluster-level dummies. However, the REWB model also includes an estimate of the between cluster effect of $x$ estimated in $\beta_2$, which is impossible in a fixed-effects model. This $\beta_2$ term is equivalent to the estimate obtained by aggregating data to the higher level and examining the relationship between the means of $x$ and $y$. Thus, this model combines the advantage of absorbing all cluster level differences when estimating individual level effects, while at the same time allowing for an analysis of the "contextual" effects of disciplines themselves.

We use this feature to estimate both within and between effects of gender and sole authorship on a scholar's *h*-index. We use 0-1 coding of the probability of a female name at the individual level to mimic the interpretation typical of a dummy variable on gender. We use the mean probability of a female name in percentage terms (0-100) at the disciplinary level to provide an estimate of the feminization of a given discipline. For sole authorship, we use the percent of a scholar's articles that are sole-authored at the individual level. At the disciplinary level, we use a natural log transformation of the mean percent sole-authorship variable because exploratory analysis indicated a negative diminishing returns relationship between sole-authorship and the *h*-index at this level. Both of the individual level variables are mean centered by discipline so that the interpretation of their effect is solely among scholars within the same discipline. The control variables of career length, specialization, and university publication count are only included as grand mean centered individual level variables. We also standardize university publication count to have a mean of zero and a standard deviation of one for ease of interpretation.

In addition to the models for the full data, we also run these same models separately for each of these fields to explore differences in patterns across fields.

# Results

We begin by analyzing a partition of the variance in the *h*-index within and between disciplines in a null multilevel model. The percentage of the total variation in the *h*-index that occurs between disciplines is given by the intraclass correlation coefficient (ICC) of this model. @tbl-partition shows the ICC for the model across all observations as well as separately by field. In total, roughly a third of the variation in the *h*-index occurs between disciplines. This substantial heterogeneity across disciplines indicates that comparing the *h*-index across disciplines is problematic because of overall differences in culture, productivity, and citation patterns across disciplines.

```{r}
#| label: tbl-partition
#| tbl-cap: Parition of variance in *h*-indexscores between and within disciplines

model_base_disc <- lmer(h_index ~ 1+(1 | discipline), data=scholars_imp[[1]])

models_base_big_field <- scholars_imp[[1]] %>%
  group_by(big_field) %>%
  group_split() %>%
  map(~ lmer(h_index ~ 1+(1 | discipline), data=.x))

models_partition <- c(model_base_disc, models_base_big_field)

tbl_partition <- map(models_partition, function(x) {
              temp <- icc_specs(x) |>
                select(grp, percent)  |>
  mutate(grp=ifelse(grp=="discipline:field", "field", grp),
         grp=factor(grp, levels=c("discipline","field","Residual"),
                    labels=c("Between discipline","Field","Within discipline")))
            }) |>
  reduce(full_join, by="grp") |>
  arrange(grp)

colnames(tbl_partition) <- c("Grouping","All", 
                             levels(scholars_imp[[1]]$big_field))

tbl_partition |> 
  gt() |>
  fmt_percent(columns = c(All, Humanities, Medical, Professional, 
                          "Social Sciences", STEM),
              decimals=1, scale=FALSE) |>
  tab_spanner("Field", columns = levels(scholars_imp[[1]]$big_field)) |>
  cols_label(`Social Sciences`="Soc. Sci", Professional="Prof.") |>
  cols_align(align="left", columns = Grouping)
```

The ICC is substantial across all fields as well, but also varies substantially. Slightly more than half the variation in the *h*-index among scholars in the humanities is between disciplines, while only one-fifth or less of the variation in the *h*-index is between disciplines among scholars in the medical, STEM, and professional fields. Some fields, like the medical, STEM, and professional fields, seem to have more shared culture and practices in terms of publishing productivity, while in other fields, we observe substantial disciplinary heterogeneity. Scholars in fields with greater commonality may mispercieve the comparability of the *h*-index across disciplines more broadly.

@tbl-models-full presents the multilevel models predicting the *h*-index across all disciplines. Model 1 predicts *h*-index scores by the individual and disciplinary variables for gender. Model 2 predicts *h*-index scores by the individual and disciplinary variables for sole authorship. Model 3 includes both sets of predictor variables from Models 1 and 2 together. Finally, Model 4 includes additional control variables for career length, specialization, and highly productive universities. The estimates for the key predictor variables of gender and sole-authorship are robust to these additional controls, although gender differences do decline in size somewhat. We use the results from Model 4 to describe overall patterns below, except where noted otherwise.

```{r}
#| label: tbl-models-full
#| tbl-cap: Multilevel models predicting *h*-index score across all disciplines with clustering at the disciplinary level.
#| results: asis

formula_pfemale <- formula(
  h_index ~ I(prop_female-disc_prop_female)+I(100*disc_prop_female)+
    (1|discipline))

formula_psole <- formula(
  h_index ~ I(100*(prop_sole-disc_prop_sole))+log(100*disc_prop_sole)+
    (1|discipline))

# main effects are group mean centered
formula_disc <- formula(
  h_index ~ 
    I(prop_female-disc_prop_female)+I(100*disc_prop_female)+
    +I(100*(prop_sole-disc_prop_sole))+log(100*disc_prop_sole)+
    (1|discipline))

# add a model with controls for age, specialization and the number of elite 
# scholars per university (all grand mean centered)
formula_controls <- update(formula_disc, .~.+I(age-mean(age))+
                             I(specialization-mean(specialization))+
                             scale(uni_pub_cnt))

# creat coef labels for most complex model
coef_labels <- c("Intercept",
                 "Prob. [0-1] of female name$\\dagger$",
                 "Disc. mean percent female name",
                 "Percent sole authored pubs$\\dagger$",
                 "Disc. mean percent sole authored (log)",
                 "Career length*",
                 "Specialization [0-1]*",
                 "Highly productive uni. count (stdized)*")

models_wb_pfemale <- map(scholars_imp, ~lmer(formula_pfemale, data=.x))
models_wb_psole <- map(scholars_imp, ~lmer(formula_psole, data=.x))
models_wb_disc <- map(scholars_imp, ~lmer(formula_disc, data=.x))
models_wb_controls <- map(scholars_imp, ~lmer(formula_controls, data=.x))

results <- lapply(list(models_wb_pfemale, models_wb_psole, models_wb_disc, 
                       models_wb_controls), pool_models)

texreg(lapply(results, convert_model), 
       custom.coef.names = coef_labels,
       digits=2,
       stars=c(0.05),
       float.pos="!t",
       caption.above = TRUE,
       custom.note="%stars. Standard errors shown in parenthesis. $\\dagger$=discipline mean centered; *=grand mean centered.")
```

Consistent with the productivity puzzle, female scholars have an *h*-index approximately 2.6 points lower than male scholars in the same discipline, on average. More frequent sole-authorship is also associated with a lower *h*-index. Within the same discipline, a one percentage point increase in the percent of sole-authored publications for a given scholar is associated with a 0.47 lower *h*-index score, on average.

Additionally, we find substantial differences between disciplines in *h*-index scores based on the feminization of the discipline and sole-authorship norms. Sole authorship behaves as expected. A one percent increase in the mean percent sole-authored publications in a discipline is associated with a 0.135 decline in the mean *h*-index for that discipline. Thus, cultural norms of more sole-authorship within a discipline contribute to lower overall *h*-index scores for that discipline.

The feminization of a discipline as indicated by the mean percent female name within the discipline has a more complex relationship to *h*-index scores. Model 1 shows a slightly negative relationship between percent female and mean *h*-index scores across disciplines. However, once the sole-authored variables were added to the model, this relationship reverses direction and becomes slightly positive. In the final model, we estimate that a one percentage point increase in the percent female name within a discipline is associated with a 0.11 increase in the mean *h*-index score for a discipline. The underlying issue is that highly feminized disciplines also tend to be more focused on sole-authorship ($r=0.30$). After accounting for this disadvantage, feminized disciplines actually have a slight advantage, although within disciplines, women are still disadvantaged relative to men.

@tbl-models-field presents models equivalent to Model 4 in @tbl-models-full, but separated by field. The most notable change in these sets of models is that there is no observable effect of disciplinary feminization within these fields, whereas we observed a moderate positive effect of disciplinary feminization across all disciplines. This finding implies that the slight positive effect of disciplinary feminization was driven by compositional issues between fields and differences in overall field productivity. The remaining variables are consistent in direction, but vary substantially in magnitude across fields. The within-discipline differences by gender are smallest in the humanities and STEM fields and largest in the medical field. Similarly, the within-discipline differences by sole-authorship are smallest in the humanities and largest in the medical field. The between-discipline effect of sole-authorship is largest in the medical field and smallest in the professional field where it is only a third as large. Overall, the heterogeneity of these effects across fields indicates another problem of comparability when attempting to compare scholars across disciplines and fields.

```{r}
#| label: tbl-models-field
#| tbl-cap: Multilevel models predicting *h*-indexscore within each field. Clustering at the disciplinary level.
#| results: asis

big_fields <- levels(scholars_imp[[1]]$big_field)
models_full_big_field <- map(big_fields, function(x) {
  models <- map(scholars_imp, function(df) {
    df <- df |>
      filter(big_field==x)
    return(lmer(formula_controls, data=df))
  })
  return(pool_models(models))
})

texreg(lapply(models_full_big_field, convert_model), 
       custom.coef.names = coef_labels,
       custom.model.names = c("Humanities","Medical","Prof.", "Soc. Sci.","STEM"),
       stars=c(0.05),
       float.pos="!t",
       caption.above = TRUE,
       custom.note="%stars. Standard errors shown in parenthesis. $\\dagger$=discipline mean centered; *=grand mean centered.")  
```

# Conclusion

The *h*-index, or Hirsch Index, is a widely used metric used for performance evaluation or quality valuation in the sciences and across the academy. This research aimed to contribute to the literature on bibliometrics and inequality in science by examining both within and between discipline differences in the *h*-index. We used REWB models to predict the *h*-index for high-performing scholars in 174 disciplines. Results indicate that gender and sole authorship affect the *h*-index providing support for our female penalty (H1) and sole authorship hypotheses (H2). We find evidence of between discipline differences as well. Results provide support for the disciplinary differences hypothesis (H3a), although the extent of disciplinary differences varies by field. Significant differences also exist between disciplines in terms of sole authorship, supporting the teamwork variation hypothesis (H3c). Support for the feminized discipline hypothesis (H3b) is mixed. Results indicate that a small negative feminized discipline effect reverses when including sole authorship and that between discipline effects likely result from compositional differences across fields confirming the field variation hypothesis (H4). In sum, important differences exist within and between disciplines in the *h*-index. Moreover, this analysis provides evidence that across field evaluation with the *h*-index risks apples-to-oranges comparison.

Data limitations suggest several avenues for future research. First, the data select on high-performing scholars, and, therefore do not generalize to the broader population of academics. While we believe that this limitation likely results in a conservative estimate of within and between discipline inequality, <!-- removed the explanatory clause here because I think its a bit speculative --> more data on academia writ large are required to verify this claim. Second, the estimation of gender suffers from well-known limitations. Data linking self-reported gender beyond the gender binary to citation data would be a welcome resource. Gender estimation also required reducing the dataset by country. Third, and along similar lines, most scholarship on publication and inequality focuses on gender as it is possible, despite these weaknesses to infer gender from names and other aspects of inequality, like race and class, remain under-studied. Future research would benefit from data linking self-reported race and class to large bibliometric data to better understand the broad effects of inequality in academic work.

In conclusion, this analysis provides further evidence that metrics in performance evaluation in academia should be used with caution, if at all. Metrics are subject to within and between discipline biases that severely hinder their value in making both intradisciplinary and interdisciplinary comparisons. Of course, these comparisons are exactly what the quantification of scholarly work proposes to facilitate. Like other forms of quantification, academic metrics simplify complex processes at a cost. This cost can reinforce existing inequalities and even generate new "automated inequalities" [@eubanks_automating_2018]. Those using metrics for evaluation, as @koopman_galton_2023 [p. 16] writes about data usage generally, "need to be fervently attentive to the ways in which inequalities may be designed into their data." In this vein, this analysis provided evidence of within and between discipline differences in the *h*-index with a focus on gender, but there is reason to believe that the bias embedded in scholarly metrics extends to other dimensions of power, including race, class, and sexual orientation.
